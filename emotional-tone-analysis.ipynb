{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets accelerate evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Disable parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Check for GPU availability and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"train_stockemo.csv\")\n",
    "\n",
    "# Print column names for debugging\n",
    "print(\"Dataset Columns:\", df.columns)\n",
    "\n",
    "# Ensure column names are lowercase and strip whitespace\n",
    "df.columns = df.columns.str.lower().str.strip()\n",
    "\n",
    "# Verify required columns\n",
    "required_columns = {\"processed\", \"emo_label\"}\n",
    "if not required_columns.issubset(df.columns):\n",
    "    raise ValueError(f\"Dataset is missing required columns. Found: {df.columns}\")\n",
    "\n",
    "# Define new label mapping\n",
    "label_merging = {\n",
    "    \"ambiguous\": \"neutral_belief\",\n",
    "    \"belief\": \"neutral_belief\",\n",
    "    \"amusement\": \"positive_outlook\",\n",
    "    \"excitement\": \"positive_outlook\",\n",
    "    \"optimism\": \"positive_outlook\",\n",
    "    \"anger\": \"anger_disgust\",\n",
    "    \"disgust\": \"anger_disgust\",\n",
    "    \"anxiety\": \"anxiety_uncertainty\",\n",
    "    \"panic\": \"anxiety_uncertainty\",\n",
    "    \"confusion\": \"anxiety_uncertainty\",\n",
    "    \"depression\": \"sadness_depression\",\n",
    "    \"surprise\": \"surprise\"\n",
    "}\n",
    "\n",
    "# Apply mapping to dataset\n",
    "df[\"merged_label\"] = df[\"emo_label\"].map(label_merging)\n",
    "\n",
    "# Ensure you have nltk wordnet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Data Augmentation Techniques\n",
    "def synonym_replacement(sentence, n=2):\n",
    "    \"\"\"Replace n words in the sentence with their synonyms.\"\"\"\n",
    "    words = sentence.split()\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        word = random.choice(words)\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        if synonyms:\n",
    "            synonym = synonyms[0].lemmas()[0].name()\n",
    "            new_words = [synonym if w == word else w for w in new_words]\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "def random_deletion(sentence, p=0.2):\n",
    "    \"\"\"Randomly remove words with probability p.\"\"\"\n",
    "    words = sentence.split()\n",
    "    if len(words) == 1:\n",
    "        return sentence\n",
    "    new_words = [word for word in words if random.random() > p]\n",
    "    return \" \".join(new_words) if new_words else words[0]\n",
    "\n",
    "def word_swap(sentence, n=2):\n",
    "    \"\"\"Randomly swap two words in the sentence n times.\"\"\"\n",
    "    words = sentence.split()\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "        new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "def augment_data(df, category, num_samples=500):\n",
    "    \"\"\"Generate new samples for a given category using augmentation techniques.\"\"\"\n",
    "    subset = df[df[\"merged_label\"] == category].copy()\n",
    "    new_samples = []\n",
    "\n",
    "    while len(new_samples) < num_samples:\n",
    "        row = subset.sample(n=1).iloc[0]\n",
    "        text = row[\"processed\"]\n",
    "\n",
    "        augmentation_choice = random.choice([synonym_replacement, random_deletion, word_swap])\n",
    "        new_text = augmentation_choice(text)\n",
    "\n",
    "        new_samples.append({\"processed\": new_text, \"merged_label\": category})\n",
    "\n",
    "    return pd.DataFrame(new_samples)\n",
    "\n",
    "# Apply augmentation to underrepresented categories\n",
    "augmented_data = pd.concat([\n",
    "    augment_data(df, \"surprise\", 1000),\n",
    "    augment_data(df, \"sadness_depression\", 1000),\n",
    "    augment_data(df, \"neutral_belief\", 500),\n",
    "    augment_data(df, \"anger_disgust\", 500),\n",
    "    augment_data(df, \"anxiety_uncertainty\", 200)\n",
    "])\n",
    "\n",
    "# Merge augmented data with original\n",
    "df = pd.concat([df, augmented_data])\n",
    "\n",
    "# Create new label-to-ID mapping\n",
    "new_labels = sorted(df[\"merged_label\"].unique())  # Sort for consistency\n",
    "label2id = {label: idx for idx, label in enumerate(new_labels)}\n",
    "df[\"label\"] = df[\"merged_label\"].map(label2id)\n",
    "\n",
    "# Print category distribution after augmentation\n",
    "print(\"Updated Emotion Category Distribution:\")\n",
    "print(df[\"merged_label\"].value_counts())\n",
    "\n",
    "# Save the augmented dataset\n",
    "df.to_csv(\"train_stockemo_augmented.csv\", index=False)\n",
    "\n",
    "# Optional: Display as a bar chart\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "df[\"merged_label\"].value_counts().plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"black\")\n",
    "plt.title(\"Emotion Category Distribution (After Augmentation)\")\n",
    "plt.xlabel(\"Emotion Category\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Print new label mapping for reference\n",
    "print(\"New Label Mapping:\", label2id)\n",
    "\n",
    "# Convert to Hugging Face Dataset (keeping only necessary columns)\n",
    "dataset = Dataset.from_pandas(df[[\"processed\", \"label\"]])\n",
    "\n",
    "# Load RoBERTa tokenizer\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"processed\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Split dataset into training (80%) and test (20%)\n",
    "tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "\n",
    "# Remove text column after tokenization\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"processed\"])\n",
    "\n",
    "print(\"Dataset successfully tokenized and split!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model & Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load model with correct number of labels\n",
    "num_labels = len(label2id)  # Get total number of emotion labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "\n",
    "# Move model to GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./roberta_emotion\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True if torch.cuda.is_available() else False,  # Use fp16 only if GPU is available\n",
    ")\n",
    "\n",
    "# Define evaluation metrics (Accuracy & F1-score)\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy.compute(predictions=predictions, references=labels)\n",
    "    f1_score = f1.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    return {\"accuracy\": acc[\"accuracy\"], \"f1\": f1_score[\"f1\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import Trainer, TrainerCallback\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Custom progress bar callback\n",
    "class AccuracyProgressBar(TrainerCallback):\n",
    "    def __init__(self, total_steps):\n",
    "        self.total_steps = total_steps  # Total number of batches\n",
    "        self.pbar = tqdm(total=total_steps, desc=\"Training Progress\", unit=\"step\")\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"Update after each training step (batch).\"\"\"\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        avg_time_per_step = elapsed_time / (state.global_step if state.global_step > 0 else 1)\n",
    "        eta = avg_time_per_step * (self.total_steps - state.global_step)\n",
    "\n",
    "        self.pbar.update(1)\n",
    "        self.pbar.set_postfix(ETA=f\"{eta:.2f}s\")\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        \"\"\"Log accuracy in real-time during evaluation.\"\"\"\n",
    "        if \"eval_accuracy\" in metrics:\n",
    "            acc = metrics[\"eval_accuracy\"]\n",
    "            tqdm.write(f\"\\n📊 Accuracy: {acc:.4f}\")\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"Close progress bar at the end of training.\"\"\"\n",
    "        self.pbar.close()\n",
    "\n",
    "# Ensure tokenizer and model are moved to GPU if available\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", use_fast=True)\n",
    "\n",
    "# Calculate the number of training steps (batches) based on the dataset and batch size\n",
    "total_steps = len(tokenized_datasets[\"train\"]) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model.to(device),  # Ensure model is on the correct device\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    # Use processing_class instead of tokenizer to avoid deprecation warning\n",
    "    processing_class=tokenizer,  # Address the deprecation warning\n",
    ")\n",
    "\n",
    "# Attach the custom progress bar callback\n",
    "trainer.add_callback(AccuracyProgressBar(total_steps))\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_results)\n",
    "\n",
    "# Save fine-tuned model\n",
    "trainer.save_model(\"./roberta_emotion_model\")\n",
    "tokenizer.save_pretrained(\"./roberta_emotion_model\")\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load fine-tuned RoBERTa model\n",
    "classifier = pipeline(\"text-classification\", model=\"./roberta_emotion_model\", tokenizer=\"./roberta_emotion_model\")\n",
    "\n",
    "# Test with a sample input\n",
    "text = \"The stock market crash has caused widespread panic among investors.\"\n",
    "result = classifier(text)\n",
    "\n",
    "print(\"Prediction:\", result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
