{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in /opt/homebrew/lib/python3.13/site-packages (0.2.54)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /opt/homebrew/lib/python3.13/site-packages (from yfinance) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/homebrew/lib/python3.13/site-packages (from yfinance) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.31 in /opt/homebrew/lib/python3.13/site-packages (from yfinance) (2.32.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /opt/homebrew/lib/python3.13/site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /Users/alyssaspasic/Library/Python/3.13/lib/python/site-packages (from yfinance) (4.3.6)\n",
      "Requirement already satisfied: pytz>=2022.5 in /opt/homebrew/lib/python3.13/site-packages (from yfinance) (2025.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /opt/homebrew/lib/python3.13/site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /opt/homebrew/lib/python3.13/site-packages (from yfinance) (3.17.9)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /opt/homebrew/lib/python3.13/site-packages (from yfinance) (4.13.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/lib/python3.13/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/homebrew/lib/python3.13/site-packages (from beautifulsoup4>=4.11.1->yfinance) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/alyssaspasic/Library/Python/3.13/lib/python/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.13/site-packages (from pandas>=1.3.0->yfinance) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.13/site-packages (from requests>=2.31->yfinance) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.13/site-packages (from requests>=2.31->yfinance) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.13/site-packages (from requests>=2.31->yfinance) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.13/site-packages (from requests>=2.31->yfinance) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in /Users/alyssaspasic/Library/Python/3.13/lib/python/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.13/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.13/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.13/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.13/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.13/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.13/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/homebrew/lib/python3.13/site-packages (4.13.3)\n",
      "Requirement already satisfied: openai in /opt/homebrew/lib/python3.13/site-packages (1.65.4)\n",
      "Requirement already satisfied: lxml in /opt/homebrew/lib/python3.13/site-packages (5.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.13/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.13/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.13/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.13/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/lib/python3.13/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/homebrew/lib/python3.13/site-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/lib/python3.13/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/lib/python3.13/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/homebrew/lib/python3.13/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/homebrew/lib/python3.13/site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/lib/python3.13/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/homebrew/lib/python3.13/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/homebrew/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: newspaper3k in /opt/homebrew/lib/python3.13/site-packages (0.2.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /opt/homebrew/lib/python3.13/site-packages (from newspaper3k) (4.13.3)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /opt/homebrew/lib/python3.13/site-packages (from newspaper3k) (11.0.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in /opt/homebrew/lib/python3.13/site-packages (from newspaper3k) (6.0.2)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in /opt/homebrew/lib/python3.13/site-packages (from newspaper3k) (1.2.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in /opt/homebrew/lib/python3.13/site-packages (from newspaper3k) (5.3.1)\n",
      "Requirement already satisfied: nltk>=3.2.1 in /opt/homebrew/lib/python3.13/site-packages (from newspaper3k) (3.9.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in /opt/homebrew/lib/python3.13/site-packages (from newspaper3k) (2.32.3)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in /opt/homebrew/lib/python3.13/site-packages (from newspaper3k) (6.0.11)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in /opt/homebrew/lib/python3.13/site-packages (from newspaper3k) (5.1.3)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in /opt/homebrew/lib/python3.13/site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in /opt/homebrew/lib/python3.13/site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/alyssaspasic/Library/Python/3.13/lib/python/site-packages (from newspaper3k) (2.9.0.post0)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in /opt/homebrew/lib/python3.13/site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/lib/python3.13/site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/homebrew/lib/python3.13/site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.12.2)\n",
      "Requirement already satisfied: six in /Users/alyssaspasic/Library/Python/3.13/lib/python/site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
      "Requirement already satisfied: sgmllib3k in /opt/homebrew/lib/python3.13/site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: click in /opt/homebrew/lib/python3.13/site-packages (from nltk>=3.2.1->newspaper3k) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.13/site-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/lib/python3.13/site-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.13/site-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.13/site-packages (from requests>=2.10.0->newspaper3k) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.13/site-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.13/site-packages (from requests>=2.10.0->newspaper3k) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.13/site-packages (from requests>=2.10.0->newspaper3k) (2025.1.31)\n",
      "Requirement already satisfied: requests-file>=1.4 in /opt/homebrew/lib/python3.13/site-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /opt/homebrew/lib/python3.13/site-packages (from tldextract>=2.0.1->newspaper3k) (3.17.0)\n",
      "zsh:1: no matches found: lxml[html_clean]\n",
      "Collecting lxml_html_clean\n",
      "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: lxml in /opt/homebrew/lib/python3.13/site-packages (from lxml_html_clean) (5.3.1)\n",
      "Downloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: lxml_html_clean\n",
      "Successfully installed lxml_html_clean-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance\n",
    "!pip install requests\n",
    "!pip install requests beautifulsoup4 openai lxml\n",
    "!pip install newspaper3k\n",
    "!pip install lxml[html_clean]\n",
    "!pip install lxml_html_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to AMZN_graph.csv\n",
      "Price        date stock_price  year optimism anxiety sadness surprise neutral  \\\n",
      "Ticker                                                                          \n",
      "0      2010-01-01      6.6950  2010        0       0       0        0       0   \n",
      "1      2010-02-01      5.9435  2010        0       0       0        0       0   \n",
      "2      2010-03-01      6.2270  2010        0       0       0        0       0   \n",
      "3      2010-04-01      6.5905  2010        0       0       0        0       0   \n",
      "4      2010-05-01      6.8745  2010        0       0       0        0       0   \n",
      "\n",
      "Price  anger_disgust  \n",
      "Ticker                \n",
      "0                  0  \n",
      "1                  0  \n",
      "2                  0  \n",
      "3                  0  \n",
      "4                  0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/wk/drd22rx56sq26c5q972b54cr0000gn/T/ipykernel_60369/2088592671.py:28: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  stock_data_monthly = stock_data.resample('MS', on='Date').first()  # 'MS' stands for Month Start\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Function to fetch stock data\n",
    "def fetch_stock_data(ticker, start_date, end_date):\n",
    "    # Fetch stock data\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    # Return only the \"Close\" prices and the Date\n",
    "    stock_data = stock_data[['Close']].reset_index()\n",
    "    return stock_data\n",
    "\n",
    "# Example: Fetch data for Apple (AAPL) from January 1, 2010 to March 1, 2025\n",
    "ticker = 'AMZN'\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2025-03-01'\n",
    "stock_data = fetch_stock_data(ticker, start_date, end_date)\n",
    "\n",
    "# Add year and stock price\n",
    "stock_data['year'] = stock_data['Date'].dt.year\n",
    "stock_data['stock_price'] = stock_data['Close']  # Renaming Close to stock_price\n",
    "\n",
    "# Initialize all emotion values to 0\n",
    "emotion_columns = ['optimism', 'anxiety', 'sadness', 'surprise', 'neutral', 'anger_disgust']\n",
    "for col in emotion_columns:\n",
    "    stock_data[col] = 0  # Set all emotions to 0 initially\n",
    "\n",
    "# Resample to get only the first trading day of each month\n",
    "stock_data_monthly = stock_data.resample('MS', on='Date').first()  # 'MS' stands for Month Start\n",
    "\n",
    "# Reset the index to bring the 'Date' back as a column\n",
    "stock_data_monthly = stock_data_monthly.reset_index()\n",
    "\n",
    "# Rename 'Date' to 'date'\n",
    "stock_data_monthly = stock_data_monthly.rename(columns={'Date': 'date'})\n",
    "\n",
    "# Select the columns to match the required format\n",
    "final_data = stock_data_monthly[['date', 'stock_price', 'year'] + emotion_columns]\n",
    "\n",
    "# Dynamically name the CSV file based on the ticker symbol\n",
    "csv_filename = f\"{ticker}_graph.csv\"\n",
    "final_data.to_csv(csv_filename, index=False)\n",
    "\n",
    "# Print the first few rows to verify the output\n",
    "print(f\"Saved to {csv_filename}\")\n",
    "print(final_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Company Name: amazon,\n",
      "Fetching news articles for amazon, (AMZN)...\n",
      "Fetching articles for 2010...\n",
      "Requesting URL: https://gnews.io/api/v4/search?q=%22amazon%2C%22&from=2010-01-01T00:00:00Z&to=2010-12-31T23:59:59Z&lang=en&max=10&token=3cef4efb93580128793bd14f13b1abf7&in=title,description\n",
      "Response status: 200\n",
      "Fetching articles for 2011...\n",
      "Requesting URL: https://gnews.io/api/v4/search?q=%22amazon%2C%22&from=2011-01-01T00:00:00Z&to=2011-12-31T23:59:59Z&lang=en&max=10&token=3cef4efb93580128793bd14f13b1abf7&in=title,description\n",
      "Response status: 200\n",
      "Fetching articles for 2012...\n",
      "Requesting URL: https://gnews.io/api/v4/search?q=%22amazon%2C%22&from=2012-01-01T00:00:00Z&to=2012-12-31T23:59:59Z&lang=en&max=10&token=3cef4efb93580128793bd14f13b1abf7&in=title,description\n",
      "Response status: 403\n",
      "Error fetching articles for 2012 (Status 403). Retrying...\n",
      "Response status: 403\n",
      "Error fetching articles for 2012 (Status 403). Retrying...\n",
      "Response status: 403\n",
      "Error fetching articles for 2012 (Status 403). Retrying...\n",
      "No articles found for 2012.\n",
      "Fetching articles for 2013...\n",
      "Requesting URL: https://gnews.io/api/v4/search?q=%22amazon%2C%22&from=2013-01-01T00:00:00Z&to=2013-12-31T23:59:59Z&lang=en&max=10&token=3cef4efb93580128793bd14f13b1abf7&in=title,description\n",
      "Response status: 403\n",
      "Error fetching articles for 2013 (Status 403). Retrying...\n",
      "Response status: 403\n",
      "Error fetching articles for 2013 (Status 403). Retrying...\n",
      "Response status: 403\n",
      "Error fetching articles for 2013 (Status 403). Retrying...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 128\u001b[39m\n\u001b[32m    125\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo articles found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompany_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 111\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    107\u001b[39m company_name = get_company_name_from_ticker(ticker_symbol) \n\u001b[32m    109\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFetching news articles for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompany_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker_symbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m articles_data = \u001b[43mfetch_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGNEWS_API_KEY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompany_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m articles_data:\n\u001b[32m    114\u001b[39m     \u001b[38;5;66;03m# Save articles to CSV with the ticker symbol in the filename\u001b[39;00m\n\u001b[32m    115\u001b[39m     filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker_symbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_news.csv\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mfetch_articles\u001b[39m\u001b[34m(api_key, company_name)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m2010\u001b[39m, current_year + \u001b[32m1\u001b[39m):\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFetching articles for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     articles = \u001b[43mfetch_articles_for_year\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompany_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m articles:\n\u001b[32m     68\u001b[39m         all_articles.extend(articles)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mfetch_articles_for_year\u001b[39m\u001b[34m(api_key, company_name, year)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     56\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError fetching articles for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (Status \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m). Retrying...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from newspaper import Article\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# Set GNews API key\n",
    "GNEWS_API_KEY = \"3cef4efb93580128793bd14f13b1abf7\"\n",
    "\n",
    "# List of common suffixes to remove\n",
    "SUFFIXES = [' Inc.', ' Ltd.', ' LLC', ' Corp.', ' Corporation', ' Co.', ' Group']\n",
    "\n",
    "# List of domain-like suffixes to remove\n",
    "DOMAIN_SUFFIXES = ['.com', '.org', '.net', '.co', '.edu']\n",
    "\n",
    "# Function to get company name from ticker symbol using Yahoo Finance\n",
    "def get_company_name_from_ticker(ticker):\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        company_name = stock.info['longName']\n",
    "        \n",
    "        # Remove common corporate suffixes from the company name\n",
    "        for suffix in SUFFIXES:\n",
    "            if company_name.endswith(suffix):\n",
    "                company_name = company_name.replace(suffix, '').strip()\n",
    "\n",
    "        # Remove domain-like suffixes (e.g., .com, .org, .net) anywhere in the company name\n",
    "        for suffix in DOMAIN_SUFFIXES:\n",
    "            company_name = company_name.lower().replace(suffix, '').strip()\n",
    "        \n",
    "        print(f\"Processed Company Name: {company_name}\")  # Debugging line to check the final name\n",
    "        return company_name\n",
    "    except KeyError:\n",
    "        print(f\"Error: Company name not found for ticker {ticker}. Using fallback name.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}. Using fallback name.\")\n",
    "        return None\n",
    "\n",
    "# Function to fetch articles for a given year\n",
    "def fetch_articles_for_year(api_key, company_name, year):\n",
    "    start_date = f'{year}-01-01T00:00:00Z'\n",
    "    end_date = f'{year}-12-31T23:59:59Z'\n",
    "    query = quote_plus(f'\"{company_name}\"')  # URL encode the company name for exact match\n",
    "    url = f'https://gnews.io/api/v4/search?q={query}&from={start_date}&to={end_date}&lang=en&max=10&token={api_key}&in=title,description'\n",
    "    \n",
    "    print(f\"Requesting URL: {url}\")  # Debugging line to print the URL\n",
    "    for _ in range(3):  # Retry up to 3 times\n",
    "        response = requests.get(url)\n",
    "        print(f\"Response status: {response.status_code}\")  # Debugging line to print the response status\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get('articles', [])\n",
    "        else:\n",
    "            print(f\"Error fetching articles for {year} (Status {response.status_code}). Retrying...\")\n",
    "            time.sleep(2)\n",
    "    return []\n",
    "\n",
    "# Function to fetch articles across multiple years\n",
    "def fetch_articles(api_key, company_name):\n",
    "    current_year = datetime.now().year\n",
    "    all_articles = []\n",
    "    for year in range(2010, current_year + 1):\n",
    "        print(f\"Fetching articles for {year}...\")\n",
    "        articles = fetch_articles_for_year(api_key, company_name, year)\n",
    "        if articles:\n",
    "            all_articles.extend(articles)\n",
    "        else:\n",
    "            print(f\"No articles found for {year}.\")\n",
    "        time.sleep(2)\n",
    "    return all_articles\n",
    "\n",
    "# Function to scrape the full content using Newspaper3k\n",
    "def scrape_full_content(url):\n",
    "    article = Article(url)\n",
    "    for _ in range(3):  # Retry up to 3 times\n",
    "        try:\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            return article.text\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {e}. Retrying...\")\n",
    "            time.sleep(2)\n",
    "    return \"Error fetching full content.\"\n",
    "\n",
    "# Function to save articles to CSV with full content\n",
    "def save_articles_to_csv_with_full_content(articles, filename):\n",
    "    headers = ['Title', 'PublishedAt', 'FullContent', 'URL']\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(headers)\n",
    "        for article in articles:\n",
    "            title = article['title']\n",
    "            published_at = article['publishedAt']\n",
    "            url = article['url']\n",
    "            full_content = scrape_full_content(url)\n",
    "            if full_content:\n",
    "                writer.writerow([title, published_at, full_content, url])\n",
    "    print(f\"Articles saved to {filename}\")\n",
    "\n",
    "# Main function to get company name and save articles\n",
    "def main():\n",
    "    ticker_symbol = input(\"Enter the ticker symbol of the company: \").upper()\n",
    "    \n",
    "    # Get company name using Yahoo Finance API \n",
    "    company_name = get_company_name_from_ticker(ticker_symbol) \n",
    "    \n",
    "    print(f\"Fetching news articles for {company_name} ({ticker_symbol})...\")\n",
    "    \n",
    "    articles_data = fetch_articles(GNEWS_API_KEY, company_name)\n",
    "    \n",
    "    if articles_data:\n",
    "        # Save articles to CSV with the ticker symbol in the filename\n",
    "        filename = f\"{ticker_symbol}_news.csv\"\n",
    "        save_articles_to_csv_with_full_content(articles_data, filename)\n",
    "        \n",
    "        # Display the first 5 articles for confirmation\n",
    "        for article in articles_data[:5]:\n",
    "            print(f\"Title: {article['title']}\")\n",
    "            print(f\"PublishedAt: {article['publishedAt']}\")\n",
    "            print(f\"URL: {article['url']}\")\n",
    "            print(f\"Full Content: {scrape_full_content(article['url'])}\\n\")\n",
    "    else:\n",
    "        print(f\"No articles found for {company_name}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
